---
title: "Preprocessing Data"
format: docx
editor: visual
---

Load libraries and split data into training and testing sets.
```{r}
# Load packages.
library(tidyverse)
library(tidymodels)

# Import soup data.
soup_data <- read_csv(here::here("Data", "soup_data.csv"))

# Split data.
soup_split <- initial_time_split(soup_data, prop = 0.90)
```

Define our recipe (outline the model).
```{r}
soup_recipe <- training(soup_split) |> 
  recipe(Sales ~ Any_Disp_Spend + Any_Price_Decr_Spend + Sub_Category)

soup_recipe
```

Handle right-skewed continuous variables.

```{r}
# Visualize Sales.
soup_data |> 
  ggplot(aes(x = Sales)) +
  geom_histogram()
```

Other skewed variables.

```{r}
# Visualize all continuous variables.
soup_data |> 
  pivot_longer(
    c(Sales, Any_Disp_Spend, Any_Price_Decr_Spend),
    names_to = "var_names",
    values_to = "cont_values"
  ) |> 
  ggplot(aes(x = cont_values)) +
  geom_histogram() + 
  facet_wrap(~ var_names)
```

Apply a log transformation and plot.

```{r}
# Use the log() transform.
soup_data |> 
  pivot_longer(
    c(Sales, Any_Disp_Spend, Any_Price_Decr_Spend),
    names_to = "var_names",
    values_to = "cont_values"
  ) |> 
  mutate(log_cont_values = log(cont_values + 1)) |> 
  ggplot(aes(x = log_cont_values)) +
  geom_histogram() + 
  facet_wrap(~ var_names, scales = "free")
```

Add the log step to the recipe.

```{r}
soup_recipe <- training(soup_split) |> 
  recipe(Sales ~ Any_Disp_Spend + Any_Price_Decr_Spend + Sub_Category) |> 
  step_log(all_numeric(), offset = 1)

soup_recipe
```

See what values exist in sub-category variable.

```{r}
soup_data |> 
  count(Sub_Category)
```

Add a step to create dummy variables for sub-category.

```{r}
soup_recipe <- training(soup_split) |> 
  recipe(Sales ~ Any_Disp_Spend + Any_Price_Decr_Spend + Sub_Category) |> 
  step_log(all_numeric(), offset = 1) |> 
  step_dummy(Sub_Category)

soup_recipe
```

Prepare the recipe.

```{r}
soup_recipe <- training(soup_split) |> 
  recipe(Sales ~ Any_Disp_Spend + Any_Price_Decr_Spend + Sub_Category) |>
  step_log(all_numeric(), offset = 1) |> 
  step_dummy(Sub_Category) |> 
  prep()

soup_recipe
```

Apply the recipe (pre-process the training and testing data sets).

```{r}
# Apply the recipe to the training data.
baked_soup_training <- soup_recipe |>
  bake(training(soup_split))

# Apply the recipe to the testing data.
baked_soup_testing <- soup_recipe |>
  bake(testing(soup_split))
```

Fit the model to the raw and pre-processed training data.

```{r}
# Fit a model without preprocessed data.
fit_raw <- linear_reg() |> 
  set_engine("lm") |> 
  fit(
    Sales ~ Any_Disp_Spend + Any_Price_Decr_Spend + Sub_Category, 
    data = training(soup_split)
  )

# Fit a model with preprocessed data.
fit_baked <- linear_reg() |> 
  set_engine("lm") |> 
  fit(
    Sales ~ ., 
    data = baked_soup_training
  )
```

Look at parameter estimates from the baked model.

```{r}
# Compare parameter estimates.
tidy(fit_baked, conf.int = TRUE) |> 
  ggplot(aes(y = term)) + 
  geom_point(aes(x = estimate)) + 
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), height = .1) +
  geom_vline(xintercept = 0, color = "red")
```

```{r}
fit_baked |>
  tidy()
```

Compute RMSE on testing data sets.

```{r}
bind_rows(
  # Compute RMSE without preprocessed testing data.
  predict(fit_raw, new_data = testing(soup_split)) |>
    bind_cols(testing(soup_split)) |>
    rmse(truth = Sales, estimate = .pred),
  # Compute RMSE with preprocessed testing data.
  predict(fit_baked, new_data = baked_soup_testing) |>
    bind_cols(baked_soup_testing) |>
    rmse(truth = Sales, estimate = .pred)
)
```

Not so fast! Reverse the log transformation on the predictions from the pre-processed test data.

```{r}
bind_rows(
  # Compute RMSE without preprocessed testing data.
  predict(fit_raw, new_data = testing(soup_split)) |>
    bind_cols(testing(soup_split)) |>
    rmse(truth = Sales, estimate = .pred),
  # Compute RMSE for predictions from preprocessed testing data...using the raw Y values
  predict(fit_baked, new_data = baked_soup_testing) |>
    bind_cols(testing(soup_split)) |>
    mutate(.pred = exp(.pred)) |>
    rmse(truth = Sales, estimate = .pred)
)
```

## Counterfactuals

Take a look at the raw data.

```{r}
soup_data |>
  select(Any_Disp_Spend, Any_Price_Decr_Spend, Sub_Category, Sales)
```

Construct the counterfactuals.

```{r}
# Counterfactual scenarios.
scenarios <- tibble(
  # compute display spend numbers, repeat them 5 times, once for each category
  Any_Disp_Spend = seq(from = 0, to = 10000, by = 2000) |> rep(5),
  # compute price decrease spend numbers, repeat them 5 times, once for each category
  Any_Price_Decr_Spend = seq(from = 10000, to = 0, by = -2000) |> rep(5),
  # character vector of subcategories, repeated six times six we have six counterfactual scenarios
  # in each sub-category
  Sub_Category = unique(soup_data$Sub_Category) |> rep(6) |> sort(),
  Sales = 1
)
```

```{r}
# Apply the recipe to the training data.
baked_scenarios <- soup_recipe |>
  bake(scenarios) |> 
  select(-Sales)

baked_scenarios
```

Generate point predictions and predictive intervals.

```{r}
# Predict and bind on prediction intervals.
bind_cols(
  predict(fit_baked, new_data = baked_scenarios),
  predict(fit_baked, new_data = baked_scenarios, type = "pred_int"),
  baked_scenarios
) |> 
  arrange(desc(.pred))
```

# Note

Preprocessing occurs in three places in our analysis.

AFTER you do the train-test split,

1. on the training data, prior to training our model.
2. on the testing data, prior to generating predictions to evaluate model fit.
3. on the counterfactuals, prior to generating counterfactual predictions

* make sure we reverse any pre-processing steps necessary for our predictions 
to be on the original scale.







