---
title: "Exercise 14"
author: "Marc Dotson"
format: docx
---

Return again to `soup_data` and the models from the previous exercises.

1. Check for right-skewed continuous variables.
2. Prepare a recipe using the necessary steps.
3. Fit the models again on the preprocessed training data.
4. Compute the RMSE again using the preprocessed testing data. Is the same model as last time still the best-fitting model?
5. Use the best-fitting model (based on RMSE) and predict `Sales` using three possible ways to split $6000 for the promotions budget. As with the training and testing data, be sure to preprocess the new data. What should the client do with their promotional budget? Provide an interpretation using dollars.
6. Render the Quarto document into Word and upload to Canvas.

**Five points total, one point each for:**

- **Visualizing right-skewed continuous variables (outcome and explanatory variables).**
- **Preparing a recipe using step_log() and fitting the models again using the preprocessed training data.**
- **Computing the RMSE using the preprocessed testing data and answering the question about how the best-fitting model compares to the previous exercise.**
- **Identifying the best-fitting model and using to predict sales on preprocessed new data and answering the question about using the hypothesized promotional budget in dollars.**
- **One point for submitting a rendered Word document.**

## Check for Right-Skewed Continuous Variables

Let's load the packages we'll need, import the data, and evaluate possible right-skewed continuous variables.

```{r}
# Load packages.
library(tidyverse)
library(tidymodels)

# Import and filter data.
soup_data <- read_csv(here::here("Data", "soup_data.csv")) |> 
  filter(Retailer_Trade_Areas == "WEST CENSUS TA", Brand_High == "CAMPBELL'S")

# Visualize the continuous variables.
soup_data |> 
  pivot_longer(
    c(Sales, Any_Disp_Spend, Any_Feat_Spend, Any_Price_Decr_Spend),
    names_to = "var_names",
    values_to = "cont_values"
  ) |> 
  ggplot(aes(x = cont_values)) +
  geom_histogram() + 
  facet_wrap(~ var_names)
```

Clearly we need to preprocess these variables by applying the log transform.

## Prepare a Recipe

Let's split the data, prepare a recipe using the training data, and apply it to both the training and testing data.

```{r}
# Split the data.
soup_split <- initial_time_split(soup_data, prop = 0.90)

# Prepare a recipe.
soup_recipe <- training(soup_split) |> 
  recipe(Sales ~ Any_Disp_Spend + Any_Feat_Spend + Any_Price_Decr_Spend) |> 
  step_log(all_numeric(), offset = 1) |> 
  prep()

# Apply the recipe to the training data.
soup_training <- soup_recipe |>
  bake(training(soup_split))

# Apply the recipe to the testing data.
soup_testing <- soup_recipe |>
  bake(testing(soup_split))
```

## Fit the Models

Now let's refit the models we ran previously, now using the preprocessed training data.

```{r}
# Full model.
fit_01 <- linear_reg() |> 
  set_engine(engine = "lm") |> 
  fit(
    Sales ~ Any_Disp_Spend + Any_Feat_Spend + Any_Price_Decr_Spend, 
    data = soup_training
  )

# Model without display spend.
fit_02 <- linear_reg() |> 
  set_engine(engine = "lm") |> 
  fit(
    Sales ~ Any_Feat_Spend + Any_Price_Decr_Spend, 
    data = soup_training
  )

# Model without feature spend.
fit_03 <- linear_reg() |> 
  set_engine(engine = "lm") |> 
  fit(
    Sales ~ Any_Disp_Spend + Any_Price_Decr_Spend, 
    data = soup_training
  )

# Model without price decrease spend.
fit_04 <- linear_reg() |> 
  set_engine(engine = "lm") |> 
  fit(
    Sales ~ Any_Disp_Spend + Any_Feat_Spend, 
    data = soup_training
  )
```

## Overall Model Fit

Now let's compute and compare RMSE using the preprocessed testing data.

```{r}
# Compute RMSE.
rmse_01 <- fit_01 |> 
  predict(new_data = soup_testing) |>
  bind_cols(soup_testing) |>
  rmse(truth = Sales, estimate = .pred)

rmse_02 <- fit_02 |> 
  predict(new_data = soup_testing) |>
  bind_cols(soup_testing) |>
  rmse(truth = Sales, estimate = .pred)

rmse_03 <- fit_03 |> 
  predict(new_data = soup_testing) |>
  bind_cols(soup_testing) |>
  rmse(truth = Sales, estimate = .pred)

rmse_04 <- fit_04 |> 
  predict(new_data = soup_testing) |>
  bind_cols(soup_testing) |>
  rmse(truth = Sales, estimate = .pred)

# Compare RMSEs.
tibble(
  model = c(
    "Full model", 
    "Model without display spend", 
    "Model without feature spend", 
    "Model without price decrease spend"
  )
) |> 
  bind_cols(
    bind_rows(
      rmse_01,
      rmse_02,
      rmse_03,
      rmse_04
    )
  ) |> 
  arrange(.estimate)
```

Based on RMSE, the best-fitting model is now the model without price decrease spend. Without the log transform, the best-fitting model was the full model.

## Predict Sales

We'll use `fit_04`, the best fitting model without price decrease spend, to predict `Sales` using three possible ways to split 6 thousand dollars for the promotions budget two ways. As with the training and testing data, we will apply the recipe to preprocess the new data.

```{r}
# Column names and preprocessing needs to match the fitted model.
scenarios <- tibble(
  Any_Disp_Spend = seq(from = 0, to = 6000, by = 3000),
  Any_Feat_Spend = seq(from = 6000, to = 0, by = -3000),
  Any_Price_Decr_Spend = 1,
  Sales = 1
)

# Apply the recipe to the training data.
scenarios <- soup_recipe |>
  bake(scenarios) |> 
  select(-c(Sales, Any_Price_Decr_Spend))

# Predict and bind on prediction intervals.
bind_cols(
  predict(fit_04, new_data = scenarios),
  predict(fit_04, new_data = scenarios, type = "pred_int"),
  scenarios
) |> 
  arrange(desc(.pred))
```

The client should split the promotional budget between display spend and feature spend. By splitting the $6000, they can expect to make, on average, $344,913 (`exp(12.75105) = 344913.90`) in the category.

